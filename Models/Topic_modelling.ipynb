{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling, Latent Dirichlet Allocation\n",
    "Topic modelling means to assign each individual item to a fixed number of topics. \n",
    "Latent Dirichlet Allocation is a method to learn the topics and assign a probability for each item belonging to that topic.\n",
    "A topic consists of a set of words found in a subset of the items. In the optimal case this set of words fit together.  \n",
    "It doesn't spit out new words not found in the input data, and it also doesn't assign one overal term to to the topic (that's what I thought first).  \n",
    "\n",
    "The goal of topic modelling for this topic is to be able to group courses and use these groupings in the recommendation.\n",
    "\n",
    "This notebook does some simple tests. It must be noted however that most of it is inspired by others work. And it is just a first version. \n",
    "\n",
    "\n",
    "The main source of inspiration was https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the needed packages\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the course info\n",
    "df=pd.read_csv('../Data/filtered_courses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use part of the course data as input data.  \n",
    "Here we only used the info from 'name' and 'content'  \n",
    "Other keys that might be used\n",
    "'name','content','learningOutcomes','additionalInformation','courseStatus','prerequisities','substitutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content']=df['content'].fillna('')\n",
    "data_text = df[['name','content']]\n",
    "df=df.astype(str)\n",
    "data_text['combined']=df[['additionalInformation', 'assesmentMethods','content','courseStatus','credits','gradingScale','learningOutcomes','level','literature','organizationId','prerequisities','teacherInCharge','type','workload']].apply(lambda x: ' '.join(x), axis=1)\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>combined</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capstone: Business Development Project</td>\n",
       "      <td>The course consists of an applied, real-life p...</td>\n",
       "      <td>Compulsory attendance in all class sessions an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction to business</td>\n",
       "      <td>This introductory course gives a basic underst...</td>\n",
       "      <td>The minimum number of participants is 20 Learn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human Resource Management</td>\n",
       "      <td>Throughout this course, we will be covering di...</td>\n",
       "      <td>Max. 100 students. Priority for management stu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Current Issues in Leadership</td>\n",
       "      <td>The course is taught by a visiting lecturer an...</td>\n",
       "      <td>nan nan The course is taught by a visiting lec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business and Society</td>\n",
       "      <td>Must know: the concepts of \"concept and contex...</td>\n",
       "      <td>nan 50% reflective learning diary50% final ess...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name  \\\n",
       "0  Capstone: Business Development Project   \n",
       "1                Introduction to business   \n",
       "2               Human Resource Management   \n",
       "3            Current Issues in Leadership   \n",
       "4                    Business and Society   \n",
       "\n",
       "                                             content  \\\n",
       "0  The course consists of an applied, real-life p...   \n",
       "1  This introductory course gives a basic underst...   \n",
       "2  Throughout this course, we will be covering di...   \n",
       "3  The course is taught by a visiting lecturer an...   \n",
       "4  Must know: the concepts of \"concept and contex...   \n",
       "\n",
       "                                            combined  index  \n",
       "0  Compulsory attendance in all class sessions an...      0  \n",
       "1  The minimum number of participants is 20 Learn...      1  \n",
       "2  Max. 100 students. Priority for management stu...      2  \n",
       "3  nan nan The course is taught by a visiting lec...      3  \n",
       "4  nan 50% reflective learning diary50% final ess...      4  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK and Gensim are both common libraries used for working with text. \n",
    "I experimented a bit with both and chose to use a mix of them. For a final version, this should be experimented more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['Compulsory', 'attendance', 'in', 'all', 'class', 'sessions', 'and', 'meetings.', 'Most', 'Master多s', 'Programme', 'studies', 'have', 'to', 'be', 'completed', 'before', 'you', 'can', 'enroll', 'on', 'the', 'Capstone', 'course.', 'The', 'maximum', 'number', 'of', 'students', 'is', '50,', 'but', 'only', 'eligible', 'candidates', 'will', 'be', 'admitted', 'even', 'if', 'the', 'maximum', 'number', 'is', 'not', 'reached.', 'Credit', 'transfer', 'and', 'capstone', 'course', 'Students', 'can,', 'on', 'legitimate', 'grounds', '(Eg.', 'exchange', 'studies', 'abroad,', 'serious', 'illness;', 'however,', 'working', 'life', 'and', 'its', 'restraints', 'are', 'not', 'considered', 'legitimate', 'reasons', 'not', 'to', 'complete', 'the', 'capstone', 'course),', 'apply', 'for', 'a', 'credit', 'transfer', 'for', 'a', 'capstone', 'course.', 'However,', 'as', 'a', 'deviation', 'from', 'the', 'common', 'process', 'of', 'credit', 'transfer', 'at', 'the', 'School', 'of', 'Business,', 'the', 'application', 'for', 'credit', 'transfer', 'must', 'be', 'submitted', 'before', 'completing', 'the', 'course', 'represented', 'to', 'substitute', 'the', 'capstone', 'course', 'at', 'the', 'School', 'of', 'Business.', 'The', 'credit', 'transfer', 'should', 'be', 'applied', 'for', 'according', 'to', 'the', 'credit', 'transfer', 'instructions', 'depicted', 'at', 'Into:', 'https://into.aalto.fi/display/enmasterbiz/Credit+transfer.', 'Based', 'on', 'the', 'credit', 'transfer', 'application,', 'the', 'teacher', 'in', 'charge', 'of', 'the', 'capstone', 'course', 'will', 'decide', 'whether', 'the', 'learning', 'goals', 'for', 'the', 'capstone', 'course', 'can', 'be', 'fulfilled', 'by', 'completing', 'the', 'course', 'the', 'student', 'has', 'represented', 'to', 'substitute', 'the', 'capstone', 'course.', 'The', 'substituting', 'course', 'cannot', 'be', 'a', 'School', 'of', 'Business', 'course.', '100', '%', 'assignments', '(group', 'and', 'individual)', 'The', 'course', 'consists', 'of', 'an', 'applied,', 'real-life', 'problem-based', 'project/case', 'that', 'students', 'identify,', 'analyze', 'and', 'solve', 'in', 'multi-disciplinary', 'teams.', 'It', 'also', 'focuses', 'on', 'developing', 'the', 'students多', 'self-awareness', 'of', 'the', 'key', 'learnings', 'during', 'their', 'studies', 'in', 'the', 'Master多s', 'Program.', 'Mandatory', 'course', 'in', 'the', 'Master多s', 'programs', 'of', 'Business', 'Law,', 'Corporate', 'Communication,', 'Entrepreneurship', '&', 'Innovation', 'Management', 'and', 'Management', '&', 'International', 'Business', '6', '0', '(fail)', '-5', '(excellent)', 'After', 'completing', 'the', 'course,', 'students', 'will', 'be', 'able', 'to', 'identify,', 'analyse', 'and', 'solve', 'real-life', 'business', 'problems', 'from', 'a', 'multi-disciplinary', 'viewpointapply', 'the', 'knowledge', 'and', 'skills', 'gained', 'during', 'their', 'studies', 'to', 'real-life', 'business', 'tasks', 'and', 'challengesmanage', 'complex', 'projects', 'and', 'work', 'in', 'diverse', 'teamspresent', 'a', 'case', 'report', 'both', 'orally', 'and', 'in', 'writingcritically', 'reflect', 'on', 'their', 'learning', 'process', 'and', 'outcomes', 'Advanced', 'specialisation', 'studies.', 'Most', 'Master多s', 'Program', 'studies', 'have', 'to', 'be', 'completed', 'before', 'one', 'can', 'enroll', 'on', 'Capstone', 'nan', 'E701', 'Most', 'Master多s', 'Programme', 'studies', 'have', 'to', 'be', 'completed', 'before', 'one', 'can', 'enroll', 'on', 'the', 'Capstone', 'course.', 'Perttu', 'K辰h辰riNina', 'GranqvistPaulina', 'JunniGregory', \"O'SheaPekka\", 'P辰lliIiris', 'Saittakari', 'course', 'Contact', 'teaching', ':10-15', 'h', '(incl.', 'closing', 'seminar', 'with', 'presentations)Independent', 'work', 'in', 'teams', 'and', 'individually:145', '-', '150', 'h']\n",
      "\n",
      "\n",
      " Preprocessed document: \n",
      "['compulsori', 'attend', 'class', 'session', 'meet', 'master', 'programm', 'studi', 'complet', 'enrol', 'capston', 'cours', 'maximum', 'number', 'student', 'elig', 'candid', 'admit', 'maximum', 'number', 'reach', 'credit', 'transfer', 'capston', 'cours', 'student', 'legitim', 'ground', 'exchang', 'studi', 'abroad', 'ill', 'work', 'life', 'restraint', 'consid', 'legitim', 'reason', 'complet', 'capston', 'cours', 'appli', 'credit', 'transfer', 'capston', 'cours', 'deviat', 'common', 'process', 'credit', 'transfer', 'school', 'busi', 'applic', 'credit', 'transfer', 'submit', 'complet', 'cours', 'repres', 'substitut', 'capston', 'cours', 'school', 'busi', 'credit', 'transfer', 'appli', 'accord', 'credit', 'transfer', 'instruct', 'depict', 'http', 'aalto', 'display', 'enmasterbiz', 'credit', 'transfer', 'base', 'credit', 'transfer', 'applic', 'teacher', 'charg', 'capston', 'cours', 'decid', 'learn', 'goal', 'capston', 'cours', 'fulfil', 'complet', 'cours', 'student', 'repres', 'substitut', 'capston', 'cours', 'substitut', 'cours', 'school', 'busi', 'cours', 'assign', 'group', 'individu', 'cours', 'consist', 'appli', 'real', 'life', 'problem', 'base', 'project', 'case', 'student', 'identifi', 'analyz', 'solv', 'multi', 'disciplinari', 'team', 'focus', 'develop', 'student', 'self', 'awar', 'key', 'learn', 'studi', 'master', 'program', 'mandatori', 'cours', 'master', 'program', 'busi', 'law', 'corpor', 'communic', 'innov', 'manag', 'manag', 'intern', 'busi', 'fail', 'excel', 'complet', 'cours', 'student', 'abl', 'identifi', 'analys', 'solv', 'real', 'life', 'busi', 'problem', 'multi', 'disciplinari', 'viewpointappli', 'knowledg', 'skill', 'gain', 'studi', 'real', 'life', 'busi', 'task', 'complex', 'project', 'work', 'divers', 'teamspres', 'case', 'report', 'oral', 'reflect', 'learn', 'process', 'outcom', 'advanc', 'specialis', 'studi', 'master', 'program', 'studi', 'complet', 'enrol', 'capston', 'nan', 'master', 'programm', 'studi', 'complet', 'enrol', 'capston', 'cours', 'perttu', 'k辰h辰rinina', 'junnigregori', 'sheapekka', 'p辰lliiiri', 'saittakari', 'cours', 'contact', 'teach', 'incl', 'close', 'seminar', 'present', 'independ', 'work', 'team', 'individu']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(doc):\n",
    "    #remove stopwords, tokenize, lemmatize and stem\n",
    "    \n",
    "    #NLTK version is bit less strict on stopwords than gensim. E.g. 'also' is not stopword in nltk\n",
    "    #stop=stopwords.words('english')\n",
    "    #gensim stopwords\n",
    "    stop=STOPWORDS\n",
    "    wnl = WordNetLemmatizer()\n",
    "    sbs=SnowballStemmer('english')\n",
    "    \n",
    "    \n",
    "    #From documentation: Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    #also tested regexptokenizer from nltk, but like outcome of this one better\n",
    "    tokens=simple_preprocess(doc)\n",
    "    \n",
    "    \n",
    "    lem=[wnl.lemmatize(t) for t in tokens if t not in stop and len(t)>=3]\n",
    "    stem=[sbs.stem(l) for l in lem]\n",
    "    return stem\n",
    "            \n",
    "processed_docs = documents['combined'].map(preprocess)\n",
    "\n",
    "#print example of original and preprocess document\n",
    "doc_sample=documents[documents['index'] == 0].values[0][2]\n",
    "words_sample=simple_preprocess(doc_sample)\n",
    "\n",
    "print('Original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n Preprocessed document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 7814\n",
      "0 aalto\n",
      "1 abl\n",
      "2 abroad\n",
      "3 accord\n",
      "4 admit\n",
      "5 advanc\n",
      "6 analys\n",
      "7 analyz\n",
      "8 appli\n",
      "9 applic\n",
      "10 assign\n"
     ]
    }
   ],
   "source": [
    "#creates set of all words in processed_docs and assigns them an 'index'(i.e. dictionary key)\n",
    "dict_docs=Dictionary(processed_docs)\n",
    "\n",
    "print(\"Number of unique tokens:\",len(dict_docs))\n",
    "\n",
    "#print first 10 entries\n",
    "count = 0\n",
    "for k, v in dict_docs.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the function below you can filter out extreme tokens\n",
    "#depends on situations whether necessary or not\n",
    "#no_below=delete words that occur less than that number, no_above=delete words that are in more than that amount of documents, keep_n is keep the most frequent n words\n",
    "#dict_docs.filter_extremes(no_above=0.1, keep_n=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the documents to a bag of words\n",
    "#this basicalyy makes dictionary for each course, of the words occuring and how often they occur\n",
    "bow_docs=[dict_docs.doc2bow(d) for d in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"aalto\") appears 2 time.\n",
      "Word 4 (\"admit\") appears 1 time.\n",
      "Word 5 (\"advanc\") appears 1 time.\n",
      "Word 8 (\"appli\") appears 1 time.\n",
      "Word 10 (\"assign\") appears 1 time.\n",
      "Word 13 (\"base\") appears 1 time.\n",
      "Word 17 (\"case\") appears 1 time.\n",
      "Word 19 (\"class\") appears 1 time.\n",
      "Word 23 (\"complet\") appears 1 time.\n",
      "Word 28 (\"contact\") appears 2 time.\n",
      "Word 29 (\"corpor\") appears 3 time.\n",
      "Word 30 (\"cours\") appears 8 time.\n",
      "Word 43 (\"exchang\") appears 1 time.\n",
      "Word 62 (\"knowledg\") appears 1 time.\n",
      "Word 68 (\"manag\") appears 2 time.\n",
      "Word 69 (\"mandatori\") appears 2 time.\n",
      "Word 71 (\"maximum\") appears 2 time.\n",
      "Word 99 (\"skill\") appears 1 time.\n",
      "Word 102 (\"student\") appears 8 time.\n",
      "Word 103 (\"studi\") appears 8 time.\n",
      "Word 107 (\"teach\") appears 1 time.\n",
      "Word 129 (\"elect\") appears 3 time.\n",
      "Word 133 (\"exam\") appears 2 time.\n",
      "Word 136 (\"financ\") appears 6 time.\n",
      "Word 137 (\"firm\") appears 3 time.\n",
      "Word 140 (\"function\") appears 1 time.\n",
      "Word 152 (\"lectur\") appears 4 time.\n",
      "Word 153 (\"market\") appears 2 time.\n",
      "Word 159 (\"particip\") appears 2 time.\n",
      "Word 160 (\"professor\") appears 2 time.\n",
      "Word 165 (\"understand\") appears 1 time.\n",
      "Word 186 (\"evalu\") appears 2 time.\n",
      "Word 190 (\"final\") appears 1 time.\n",
      "Word 192 (\"hexam\") appears 1 time.\n",
      "Word 217 (\"provid\") appears 1 time.\n",
      "Word 218 (\"read\") appears 1 time.\n",
      "Word 219 (\"recommend\") appears 1 time.\n",
      "Word 221 (\"relat\") appears 1 time.\n",
      "Word 227 (\"select\") appears 1 time.\n",
      "Word 233 (\"techniqu\") appears 1 time.\n",
      "Word 237 (\"track\") appears 2 time.\n",
      "Word 246 (\"minor\") appears 5 time.\n",
      "Word 282 (\"strategi\") appears 2 time.\n",
      "Word 286 (\"valu\") appears 1 time.\n",
      "Word 292 (\"avail\") appears 1 time.\n",
      "Word 294 (\"biz\") appears 1 time.\n",
      "Word 325 (\"inform\") appears 1 time.\n",
      "Word 342 (\"order\") appears 1 time.\n",
      "Word 355 (\"registr\") appears 3 time.\n",
      "Word 404 (\"follow\") appears 2 time.\n",
      "Word 406 (\"global\") appears 2 time.\n",
      "Word 414 (\"method\") appears 1 time.\n",
      "Word 417 (\"note\") appears 1 time.\n",
      "Word 439 (\"categori\") appears 1 time.\n",
      "Word 440 (\"cem\") appears 4 time.\n",
      "Word 469 (\"level\") appears 1 time.\n",
      "Word 470 (\"list\") appears 1 time.\n",
      "Word 471 (\"make\") appears 1 time.\n",
      "Word 477 (\"possibl\") appears 1 time.\n",
      "Word 480 (\"rais\") appears 1 time.\n",
      "Word 486 (\"tool\") appears 1 time.\n",
      "Word 487 (\"univers\") appears 1 time.\n",
      "Word 505 (\"hindepend\") appears 1 time.\n",
      "Word 521 (\"success\") appears 1 time.\n",
      "Word 559 (\"guid\") appears 1 time.\n",
      "Word 592 (\"requir\") appears 1 time.\n",
      "Word 638 (\"structur\") appears 2 time.\n",
      "Word 642 (\"ventur\") appears 2 time.\n",
      "Word 657 (\"choos\") appears 1 time.\n",
      "Word 659 (\"circul\") appears 1 time.\n",
      "Word 693 (\"opportun\") appears 2 time.\n",
      "Word 825 (\"effect\") appears 1 time.\n",
      "Word 834 (\"introduc\") appears 1 time.\n",
      "Word 874 (\"engin\") appears 4 time.\n",
      "Word 879 (\"financi\") appears 6 time.\n",
      "Word 896 (\"measur\") appears 2 time.\n",
      "Word 967 (\"decis\") appears 1 time.\n",
      "Word 971 (\"fundament\") appears 1 time.\n",
      "Word 1062 (\"introduct\") appears 1 time.\n",
      "Word 1084 (\"assist\") appears 2 time.\n",
      "Word 1085 (\"coordin\") appears 1 time.\n",
      "Word 1119 (\"cash\") appears 2 time.\n",
      "Word 1123 (\"flow\") appears 1 time.\n",
      "Word 1128 (\"invest\") appears 5 time.\n",
      "Word 1144 (\"supplementari\") appears 1 time.\n",
      "Word 1147 (\"valuat\") appears 1 time.\n",
      "Word 1155 (\"accept\") appears 1 time.\n",
      "Word 1178 (\"weboodi\") appears 1 time.\n",
      "Word 1189 (\"investor\") appears 1 time.\n",
      "Word 1199 (\"capit\") appears 4 time.\n",
      "Word 1218 (\"cost\") appears 1 time.\n",
      "Word 1289 (\"mim\") appears 1 time.\n",
      "Word 1476 (\"price\") appears 1 time.\n",
      "Word 1477 (\"return\") appears 1 time.\n",
      "Word 1498 (\"optim\") appears 1 time.\n",
      "Word 1570 (\"asset\") appears 1 time.\n",
      "Word 1626 (\"deadlin\") appears 1 time.\n",
      "Word 1804 (\"sci\") appears 1 time.\n",
      "Word 1840 (\"compar\") appears 1 time.\n",
      "Word 2039 (\"coursefinanci\") appears 1 time.\n",
      "Word 2041 (\"kaila\") appears 1 time.\n",
      "Word 2047 (\"ruth\") appears 1 time.\n",
      "Word 2095 (\"jylh辰\") appears 2 time.\n",
      "Word 2098 (\"petri\") appears 2 time.\n",
      "Word 2101 (\"care\") appears 1 time.\n",
      "Word 2107 (\"priorit\") appears 1 time.\n",
      "Word 2110 (\"riski\") appears 1 time.\n",
      "Word 2111 (\"seat\") appears 1 time.\n",
      "Word 2115 (\"confirm\") appears 1 time.\n",
      "Word 2118 (\"sign\") appears 1 time.\n",
      "Word 2138 (\"previous\") appears 2 time.\n",
      "Word 2152 (\"berk\") appears 1 time.\n",
      "Word 2153 (\"coursestrategi\") appears 2 time.\n",
      "Word 2154 (\"demarzo\") appears 1 time.\n",
      "Word 2155 (\"iem\") appears 2 time.\n",
      "Word 2156 (\"optimis\") appears 1 time.\n",
      "Word 2157 (\"welch\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#printe a sample of the bag of words of one course\n",
    "bow_doc_sample = bow_docs[100]\n",
    "for i in range(len(bow_doc_sample)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_sample[i][0], dict_docs[bow_doc_sample[i][0]], bow_doc_sample[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can input the bag of words to the LDA model and get our topics! \n",
    "#num_topics defines the number of topics it outputs\n",
    "lda_model = LdaMulticore(bow_docs, num_topics=20, id2word=dict_docs, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"nan\" + 0.027*\"cours\" + 0.020*\"student\" + 0.015*\"game\" + 0.010*\"market\" + 0.008*\"studi\" + 0.008*\"design\" + 0.008*\"lectur\" + 0.007*\"work\" + 0.007*\"exercis\"\n",
      "Topic: 1 \n",
      "Words: 0.050*\"cours\" + 0.026*\"student\" + 0.014*\"work\" + 0.012*\"lectur\" + 0.012*\"nan\" + 0.011*\"project\" + 0.011*\"design\" + 0.010*\"master\" + 0.010*\"studi\" + 0.008*\"process\"\n",
      "Topic: 2 \n",
      "Words: 0.030*\"cours\" + 0.015*\"student\" + 0.014*\"busi\" + 0.013*\"understand\" + 0.011*\"sustain\" + 0.010*\"develop\" + 0.009*\"studi\" + 0.009*\"manag\" + 0.009*\"nan\" + 0.009*\"model\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"structur\" + 0.015*\"ice\" + 0.014*\"cours\" + 0.011*\"fuel\" + 0.010*\"cell\" + 0.010*\"materi\" + 0.010*\"work\" + 0.009*\"test\" + 0.009*\"understand\" + 0.008*\"student\"\n",
      "Topic: 4 \n",
      "Words: 0.045*\"cours\" + 0.031*\"student\" + 0.022*\"skill\" + 0.018*\"work\" + 0.013*\"studi\" + 0.012*\"communic\" + 0.010*\"present\" + 0.009*\"project\" + 0.008*\"nan\" + 0.008*\"write\"\n",
      "Topic: 5 \n",
      "Words: 0.041*\"cours\" + 0.027*\"student\" + 0.025*\"nan\" + 0.015*\"research\" + 0.013*\"studi\" + 0.012*\"assign\" + 0.011*\"lectur\" + 0.010*\"materi\" + 0.010*\"design\" + 0.009*\"work\"\n",
      "Topic: 6 \n",
      "Words: 0.030*\"cours\" + 0.020*\"work\" + 0.017*\"research\" + 0.016*\"student\" + 0.015*\"nan\" + 0.014*\"thesi\" + 0.011*\"design\" + 0.011*\"seminar\" + 0.011*\"lectur\" + 0.010*\"studi\"\n",
      "Topic: 7 \n",
      "Words: 0.020*\"cours\" + 0.014*\"project\" + 0.013*\"estat\" + 0.012*\"fiber\" + 0.012*\"real\" + 0.011*\"student\" + 0.011*\"optic\" + 0.007*\"develop\" + 0.007*\"understand\" + 0.007*\"materi\"\n",
      "Topic: 8 \n",
      "Words: 0.016*\"cours\" + 0.012*\"design\" + 0.008*\"student\" + 0.005*\"work\" + 0.005*\"present\" + 0.004*\"research\" + 0.004*\"develop\" + 0.003*\"studi\" + 0.003*\"servic\" + 0.003*\"final\"\n",
      "Topic: 9 \n",
      "Words: 0.038*\"cours\" + 0.018*\"learn\" + 0.017*\"student\" + 0.014*\"nan\" + 0.014*\"work\" + 0.011*\"master\" + 0.011*\"data\" + 0.008*\"program\" + 0.008*\"level\" + 0.008*\"scienc\"\n",
      "Topic: 10 \n",
      "Words: 0.030*\"cours\" + 0.023*\"manag\" + 0.015*\"system\" + 0.013*\"design\" + 0.012*\"student\" + 0.011*\"lectur\" + 0.010*\"exam\" + 0.010*\"nan\" + 0.010*\"develop\" + 0.009*\"ship\"\n",
      "Topic: 11 \n",
      "Words: 0.035*\"cours\" + 0.035*\"student\" + 0.028*\"nan\" + 0.016*\"design\" + 0.014*\"lectur\" + 0.014*\"work\" + 0.011*\"studi\" + 0.010*\"project\" + 0.008*\"develop\" + 0.008*\"present\"\n",
      "Topic: 12 \n",
      "Words: 0.028*\"design\" + 0.026*\"cours\" + 0.019*\"nan\" + 0.013*\"master\" + 0.012*\"student\" + 0.009*\"level\" + 0.009*\"studi\" + 0.008*\"lectur\" + 0.007*\"basic\" + 0.007*\"structur\"\n",
      "Topic: 13 \n",
      "Words: 0.034*\"cours\" + 0.026*\"student\" + 0.025*\"design\" + 0.012*\"busi\" + 0.012*\"lectur\" + 0.011*\"work\" + 0.011*\"energi\" + 0.010*\"studi\" + 0.009*\"nan\" + 0.009*\"develop\"\n",
      "Topic: 14 \n",
      "Words: 0.009*\"cours\" + 0.009*\"stori\" + 0.009*\"model\" + 0.008*\"student\" + 0.007*\"formul\" + 0.007*\"understand\" + 0.007*\"propag\" + 0.007*\"estim\" + 0.006*\"structur\" + 0.006*\"strategi\"\n",
      "Topic: 15 \n",
      "Words: 0.027*\"cours\" + 0.018*\"design\" + 0.013*\"work\" + 0.012*\"exercis\" + 0.012*\"lectur\" + 0.011*\"nan\" + 0.011*\"basic\" + 0.010*\"student\" + 0.009*\"hr\" + 0.008*\"studi\"\n",
      "Topic: 16 \n",
      "Words: 0.032*\"cours\" + 0.019*\"nan\" + 0.018*\"student\" + 0.013*\"market\" + 0.010*\"art\" + 0.009*\"work\" + 0.009*\"urban\" + 0.008*\"understand\" + 0.007*\"econom\" + 0.006*\"discuss\"\n",
      "Topic: 17 \n",
      "Words: 0.021*\"cours\" + 0.015*\"element\" + 0.015*\"model\" + 0.015*\"finit\" + 0.014*\"problem\" + 0.014*\"comput\" + 0.013*\"method\" + 0.011*\"lectur\" + 0.010*\"studi\" + 0.010*\"exercis\"\n",
      "Topic: 18 \n",
      "Words: 0.035*\"cours\" + 0.023*\"nan\" + 0.017*\"design\" + 0.015*\"project\" + 0.011*\"studi\" + 0.010*\"work\" + 0.009*\"understand\" + 0.009*\"student\" + 0.008*\"process\" + 0.008*\"model\"\n",
      "Topic: 19 \n",
      "Words: 0.050*\"cours\" + 0.019*\"student\" + 0.018*\"nan\" + 0.011*\"lectur\" + 0.010*\"method\" + 0.009*\"exercis\" + 0.009*\"work\" + 0.009*\"studi\" + 0.009*\"understand\" + 0.008*\"assign\"\n"
     ]
    }
   ],
   "source": [
    "#print the words belonging to each topic! \n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.02075140837182791),\n",
      " (1, 0.013250239071957146),\n",
      " (2, 0.06835884923831546),\n",
      " (3, 0.03863250427237543),\n",
      " (4, 0.04484150580231598),\n",
      " (5, 0.012526598419456726),\n",
      " (6, 0.03570979717795061),\n",
      " (7, 0.025229956948338864),\n",
      " (8, 0.04857666181366953),\n",
      " (9, 0.043095751780433206),\n",
      " (10, 0.010295476334941161),\n",
      " (11, 0.020803080966136176),\n",
      " (12, 0.04115335036878248),\n",
      " (13, 0.029581823518127576),\n",
      " (14, 0.1622046780190968),\n",
      " (15, 0.061748314519394214),\n",
      " (16, 0.6174831451939421),\n",
      " (17, 0.051781374347831),\n",
      " (18, 0.048658016705338314),\n",
      " (19, 0.01758993002837512),\n",
      " (20, 0.055218585451640864),\n",
      " (21, 0.027569820598196168),\n",
      " (22, 0.024396081369395747),\n",
      " (23, 0.10169012631113153),\n",
      " (24, 0.03788625109486548),\n",
      " (25, 0.019285940217996796),\n",
      " (26, 0.04287292939284812),\n",
      " (27, 0.03093099638741497),\n",
      " (28, 0.015864115276122415),\n",
      " (29, 0.04235906794649344),\n",
      " (31, 0.27067019146135035),\n",
      " (32, 0.056581293025185116),\n",
      " (33, 0.0777965418015885),\n",
      " (34, 0.014824242772669523),\n",
      " (35, 0.06532059085128834),\n",
      " (36, 0.10260317826243497),\n",
      " (37, 0.055218585451640864),\n",
      " (38, 0.04546050840633197),\n",
      " (39, 0.0966719269281346),\n",
      " (40, 0.08723423436486155),\n",
      " (41, 0.14478999223257108),\n",
      " (42, 0.047506707640620524),\n",
      " (43, 0.040260196645518165),\n",
      " (44, 0.018303397114013676),\n",
      " (45, 0.023867249374794612),\n",
      " (46, 0.04610899821680192),\n",
      " (47, 0.034244329024119495),\n",
      " (48, 0.03902155108982775),\n",
      " (49, 0.05284463990098818),\n",
      " (50, 0.014045990256674308),\n",
      " (51, 0.04235906794649344),\n",
      " (52, 0.05565507786884569),\n",
      " (53, 0.07017697425776356),\n",
      " (54, 0.05809570926861136),\n",
      " (55, 0.012985276309907545),\n",
      " (56, 0.039133372999047934),\n",
      " (57, 0.028178931656365955),\n",
      " (58, 0.039834671005821),\n",
      " (59, 0.030189100935100955),\n",
      " (60, 0.0966719269281346),\n",
      " (61, 0.030822504082245118),\n",
      " (62, 0.015299652591496331),\n",
      " (63, 0.0966719269281346),\n",
      " (64, 0.050823795817897324),\n",
      " (65, 0.03264672163725899),\n",
      " (66, 0.13671769847663093),\n",
      " (67, 0.10762203104728693),\n",
      " (68, 0.03634948751506495),\n",
      " (69, 0.03510455476940438),\n",
      " (70, 0.03573793102460572),\n",
      " (71, 0.08230670073756496),\n",
      " (72, 0.034957356446478925),\n",
      " (73, 0.09501341528124105),\n",
      " (74, 0.0012954693516747972),\n",
      " (75, 0.058389623418204785),\n",
      " (76, 0.03882563818091731),\n",
      " (77, 0.02909926245902936),\n",
      " (78, 0.0817135381220119),\n",
      " (79, 0.014790911759063788),\n",
      " (80, 0.043095751780433206),\n",
      " (81, 0.0316562848709845),\n",
      " (82, 0.06163997088196256),\n",
      " (83, 0.02667943667089995),\n",
      " (84, 0.02911970340605798),\n",
      " (85, 0.0966719269281346),\n",
      " (86, 0.06532059085128834),\n",
      " (87, 0.09412146842221468),\n",
      " (88, 0.04948346411176937),\n",
      " (89, 0.026905507761821733),\n",
      " (90, 0.025815669285750138),\n",
      " (91, 0.10164759163579465),\n",
      " (92, 0.07227584555873885),\n",
      " (93, 0.0966719269281346),\n",
      " (94, 0.10576008554706955),\n",
      " (95, 0.027068572133066262),\n",
      " (96, 0.026585123279785884),\n",
      " (97, 0.023110626271224777),\n",
      " (98, 0.0966719269281346),\n",
      " (99, 0.015936347340798063),\n",
      " (100, 0.05397359167528996),\n",
      " (101, 0.06283815299546582),\n",
      " (102, 0.013292727309419956),\n",
      " (103, 0.037589121523677445),\n",
      " (104, 0.04484150580231598),\n",
      " (105, 0.20507654771494638),\n",
      " (106, 0.03267153879092349),\n",
      " (107, 0.017187068917021514),\n",
      " (108, 0.029682882324904555),\n",
      " (109, 0.061429738865449446),\n",
      " (110, 0.0966719269281346),\n",
      " (111, 0.3539954002449376),\n",
      " (112, 0.0966719269281346),\n",
      " (113, 0.016714031136018673)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_docs)\n",
    "corpus_tfidf = tfidf[bow_docs]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.002*\"art\" + 0.002*\"design\" + 0.002*\"project\" + 0.002*\"artist\" + 0.002*\"cultur\" + 0.002*\"learn\" + 0.002*\"urban\" + 0.002*\"plan\" + 0.002*\"system\" + 0.001*\"research\"\n",
      "Topic: 1 Word: 0.003*\"project\" + 0.003*\"design\" + 0.002*\"servic\" + 0.002*\"softwar\" + 0.002*\"process\" + 0.002*\"skill\" + 0.002*\"market\" + 0.002*\"manag\" + 0.002*\"research\" + 0.002*\"medium\"\n",
      "Topic: 2 Word: 0.003*\"busi\" + 0.002*\"design\" + 0.002*\"project\" + 0.002*\"model\" + 0.002*\"research\" + 0.002*\"develop\" + 0.002*\"thesi\" + 0.002*\"process\" + 0.002*\"method\" + 0.002*\"market\"\n",
      "Topic: 3 Word: 0.002*\"design\" + 0.002*\"energi\" + 0.002*\"project\" + 0.002*\"doctor\" + 0.001*\"research\" + 0.001*\"mechan\" + 0.001*\"scienc\" + 0.001*\"exercis\" + 0.001*\"system\" + 0.001*\"method\"\n",
      "Topic: 4 Word: 0.008*\"nan\" + 0.003*\"game\" + 0.002*\"data\" + 0.002*\"busi\" + 0.002*\"design\" + 0.002*\"research\" + 0.002*\"financ\" + 0.002*\"manag\" + 0.002*\"model\" + 0.002*\"project\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dict_docs, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Can see some structure, but at the same time also quite random words in the same topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Try different methods\n",
    "    - Try with TF-IDF instead of BOW\n",
    "    \n",
    "- What can be the inputs for this method? Should it state something about the topic? Would say so?\n",
    "    - Is this a difference compared to e.g. Word2Vec, where it places an entry in a vector space and hence could in theory have any information we find important (e.g. teacher)\n",
    "        - Guess then it is not really topic modelling anymore\n",
    "- Experiment more with different input data\n",
    "- Be aware of the differences between NLTK vs Gensim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
