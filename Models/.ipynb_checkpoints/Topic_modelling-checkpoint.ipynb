{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling, Latent Dirichlet Allocation\n",
    "Topic modelling means to assign each individual item to a fixed number of topics. \n",
    "Latent Dirichlet Allocation is a method to learn the topics and assign a probability for each item belonging to that topic.\n",
    "A topic consists of a set of words found in a subset of the items. In the optimal case this set of words fit together.  \n",
    "It doesn't spit out new words not found in the input data, and it also doesn't assign one overal term to to the topic (that's what I thought first).  \n",
    "\n",
    "The goal of topic modelling for this topic is to be able to group courses and use these groupings in the recommendation.\n",
    "\n",
    "This notebook does some simple tests. It must be noted however that most of it is inspired by others work. And it is just a first version. \n",
    "\n",
    "\n",
    "The main source of inspiration was https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the needed packages\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the course info\n",
    "df=pd.read_csv('../Data/filtered_courses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use part of the course data as input data.  \n",
    "Here we only used the info from 'name' and 'content'  \n",
    "Other keys that might be used\n",
    "'name','content','learningOutcomes','additionalInformation','courseStatus','prerequisities','substitutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tinka/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/tinka/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df['content']=df['content'].fillna('')\n",
    "data_text = df[['name','content']]\n",
    "data_text['combined']=df[['name', 'content']].apply(lambda x: ' '.join(x), axis=1)\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>combined</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capstone: Business Development Project</td>\n",
       "      <td>The course consists of an applied, real-life p...</td>\n",
       "      <td>Capstone: Business Development Project The cou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction to business</td>\n",
       "      <td>This introductory course gives a basic underst...</td>\n",
       "      <td>Introduction to business This introductory cou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human Resource Management</td>\n",
       "      <td>Throughout this course, we will be covering di...</td>\n",
       "      <td>Human Resource Management Throughout this cour...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Current Issues in Leadership</td>\n",
       "      <td>The course is taught by a visiting lecturer an...</td>\n",
       "      <td>Current Issues in Leadership The course is tau...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business and Society</td>\n",
       "      <td>Must know: the concepts of \"concept and contex...</td>\n",
       "      <td>Business and Society Must know: the concepts o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name  \\\n",
       "0  Capstone: Business Development Project   \n",
       "1                Introduction to business   \n",
       "2               Human Resource Management   \n",
       "3            Current Issues in Leadership   \n",
       "4                    Business and Society   \n",
       "\n",
       "                                             content  \\\n",
       "0  The course consists of an applied, real-life p...   \n",
       "1  This introductory course gives a basic underst...   \n",
       "2  Throughout this course, we will be covering di...   \n",
       "3  The course is taught by a visiting lecturer an...   \n",
       "4  Must know: the concepts of \"concept and contex...   \n",
       "\n",
       "                                            combined  index  \n",
       "0  Capstone: Business Development Project The cou...      0  \n",
       "1  Introduction to business This introductory cou...      1  \n",
       "2  Human Resource Management Throughout this cour...      2  \n",
       "3  Current Issues in Leadership The course is tau...      3  \n",
       "4  Business and Society Must know: the concepts o...      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK and Gensim are both common libraries used for working with text. \n",
    "I experimented a bit with both and chose to use a mix of them. For a final version, this should be experimented more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['Capstone:', 'Business', 'Development', 'Project', 'The', 'course', 'consists', 'of', 'an', 'applied,', 'real-life', 'problem-based', 'project/case', 'that', 'students', 'identify,', 'analyze', 'and', 'solve', 'in', 'multi-disciplinary', 'teams.', 'It', 'also', 'focuses', 'on', 'developing', 'the', 'students¿', 'self-awareness', 'of', 'the', 'key', 'learnings', 'during', 'their', 'studies', 'in', 'the', 'Master¿s', 'Program.']\n",
      "\n",
      "\n",
      " Preprocessed document: \n",
      "['capston', 'busi', 'develop', 'project', 'cours', 'consist', 'appli', 'real', 'life', 'problem', 'base', 'project', 'case', 'student', 'identifi', 'analyz', 'solv', 'multi', 'disciplinari', 'team', 'focus', 'develop', 'student', 'self', 'awar', 'key', 'learn', 'studi', 'master', 'program']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(doc):\n",
    "    #remove stopwords, tokenize, lemmatize and stem\n",
    "    \n",
    "    #NLTK version is bit less strict on stopwords than gensim. E.g. 'also' is not stopword in nltk\n",
    "    #stop=stopwords.words('english')\n",
    "    #gensim stopwords\n",
    "    stop=STOPWORDS\n",
    "    wnl = WordNetLemmatizer()\n",
    "    sbs=SnowballStemmer('english')\n",
    "    \n",
    "    \n",
    "    #From documentation: Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    #also tested regexptokenizer from nltk, but like outcome of this one better\n",
    "    tokens=simple_preprocess(doc)\n",
    "    \n",
    "    \n",
    "    lem=[wnl.lemmatize(t) for t in tokens if t not in stop and len(t)>=3]\n",
    "    stem=[sbs.stem(l) for l in lem]\n",
    "    return stem\n",
    "            \n",
    "processed_docs = documents['combined'].map(preprocess)\n",
    "\n",
    "#print example of original and preprocess document\n",
    "doc_sample=documents[documents['index'] == 0].values[0][2]\n",
    "words_sample=simple_preprocess(doc_sample)\n",
    "\n",
    "print('Original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n Preprocessed document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6165\n",
      "0 analyz\n",
      "1 appli\n",
      "2 awar\n",
      "3 base\n",
      "4 busi\n",
      "5 capston\n",
      "6 case\n",
      "7 consist\n",
      "8 cours\n",
      "9 develop\n",
      "10 disciplinari\n"
     ]
    }
   ],
   "source": [
    "#creates set of all words in processed_docs and assigns them an 'index'(i.e. dictionary key)\n",
    "dict_docs=Dictionary(processed_docs)\n",
    "\n",
    "print(\"Number of unique tokens:\",len(dict_docs))\n",
    "\n",
    "#print first 10 entries\n",
    "count = 0\n",
    "for k, v in dict_docs.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the function below you can filter out extreme tokens\n",
    "#depends on situations whether necessary or not\n",
    "#no_below=delete words that occur less than that number, no_above=delete words that are in more than that amount of documents, keep_n is keep the most frequent n words\n",
    "#dict_docs.filter_extremes(no_above=0.1, keep_n=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the documents to a bag of words\n",
    "#this basicalyy makes dictionary for each course, of the words occuring and how often they occur\n",
    "bow_docs=[dict_docs.doc2bow(d) for d in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 47 (\"manag\") appears 1 time.\n",
      "Word 115 (\"corpor\") appears 1 time.\n",
      "Word 259 (\"structur\") appears 1 time.\n",
      "Word 265 (\"choos\") appears 1 time.\n",
      "Word 283 (\"opportun\") appears 1 time.\n",
      "Word 502 (\"cash\") appears 2 time.\n",
      "Word 508 (\"financi\") appears 1 time.\n",
      "Word 509 (\"flow\") appears 1 time.\n",
      "Word 511 (\"invest\") appears 4 time.\n",
      "Word 514 (\"measur\") appears 2 time.\n",
      "Word 521 (\"valuat\") appears 1 time.\n",
      "Word 536 (\"investor\") appears 1 time.\n",
      "Word 545 (\"capit\") appears 1 time.\n",
      "Word 720 (\"return\") appears 1 time.\n",
      "Word 1080 (\"riski\") appears 1 time.\n",
      "Word 1115 (\"optimis\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#printe a sample of the bag of words of one course\n",
    "bow_doc_sample = bow_docs[100]\n",
    "for i in range(len(bow_doc_sample)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_sample[i][0], dict_docs[bow_doc_sample[i][0]], bow_doc_sample[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can input the bag of words to the LDA model and get our topics! \n",
    "#num_topics defines the number of topics it outputs\n",
    "lda_model = LdaMulticore(bow_docs, num_topics=10, id2word=dict_docs, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.020*\"cours\" + 0.008*\"busi\" + 0.008*\"analysi\" + 0.008*\"student\" + 0.007*\"data\" + 0.007*\"concret\" + 0.007*\"materi\" + 0.006*\"model\" + 0.006*\"manag\" + 0.006*\"topic\"\n",
      "Topic: 1 \n",
      "Words: 0.009*\"work\" + 0.009*\"cours\" + 0.009*\"art\" + 0.007*\"design\" + 0.007*\"theori\" + 0.007*\"model\" + 0.007*\"project\" + 0.006*\"invest\" + 0.006*\"student\" + 0.005*\"build\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"cours\" + 0.012*\"model\" + 0.009*\"manag\" + 0.009*\"market\" + 0.009*\"busi\" + 0.008*\"method\" + 0.007*\"analysi\" + 0.007*\"develop\" + 0.006*\"strategi\" + 0.006*\"design\"\n",
      "Topic: 3 \n",
      "Words: 0.020*\"student\" + 0.013*\"design\" + 0.012*\"cours\" + 0.010*\"process\" + 0.009*\"work\" + 0.009*\"research\" + 0.009*\"topic\" + 0.008*\"project\" + 0.007*\"seminar\" + 0.007*\"manag\"\n",
      "Topic: 4 \n",
      "Words: 0.027*\"cours\" + 0.025*\"student\" + 0.019*\"design\" + 0.014*\"skill\" + 0.013*\"research\" + 0.011*\"work\" + 0.007*\"busi\" + 0.007*\"technolog\" + 0.007*\"present\" + 0.007*\"develop\"\n",
      "Topic: 5 \n",
      "Words: 0.021*\"structur\" + 0.017*\"method\" + 0.013*\"element\" + 0.011*\"finit\" + 0.010*\"materi\" + 0.008*\"data\" + 0.008*\"topic\" + 0.007*\"analysi\" + 0.006*\"theori\" + 0.006*\"basic\"\n",
      "Topic: 6 \n",
      "Words: 0.033*\"project\" + 0.021*\"cours\" + 0.020*\"student\" + 0.012*\"manag\" + 0.012*\"research\" + 0.011*\"develop\" + 0.010*\"model\" + 0.009*\"busi\" + 0.009*\"work\" + 0.009*\"seminar\"\n",
      "Topic: 7 \n",
      "Words: 0.032*\"cours\" + 0.027*\"design\" + 0.010*\"student\" + 0.009*\"work\" + 0.009*\"process\" + 0.009*\"learn\" + 0.008*\"system\" + 0.008*\"method\" + 0.007*\"practic\" + 0.007*\"model\"\n",
      "Topic: 8 \n",
      "Words: 0.029*\"cours\" + 0.014*\"student\" + 0.013*\"practic\" + 0.013*\"manag\" + 0.010*\"cultur\" + 0.009*\"research\" + 0.009*\"art\" + 0.007*\"includ\" + 0.007*\"communic\" + 0.006*\"design\"\n",
      "Topic: 9 \n",
      "Words: 0.019*\"cours\" + 0.013*\"student\" + 0.011*\"develop\" + 0.010*\"process\" + 0.009*\"design\" + 0.008*\"topic\" + 0.008*\"product\" + 0.008*\"wood\" + 0.007*\"work\" + 0.006*\"research\"\n"
     ]
    }
   ],
   "source": [
    "#print the words belonging to each topic! \n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Can see some structure, but at the same time also quite random words in the same topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Try different methods\n",
    "    - Try with TF-IDF instead of BOW\n",
    "    \n",
    "- What can be the inputs for this method? Should it state something about the topic? Would say so?\n",
    "    - Is this a difference compared to e.g. Word2Vec, where it places an entry in a vector space and hence could in theory have any information we find important (e.g. teacher)\n",
    "        - Guess then it is not really topic modelling anymore\n",
    "- Experiment more with different input data\n",
    "- Be aware of the differences between NLTK vs Gensim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
